# simpleGEMM

| ![img-uNPCSD5UDSQHHgRJpcDy7Lzf](https://github.com/andylolu2/simpleGEMM/assets/66584117/5def8c80-9e51-49ee-ba1f-9538e072083e) | 
|:--:| 
| *Generated by DALLÂ·E 3* |

This is an *extremely* minimalistic but fast implementation of matrix multiplication in CUDA. The source code is a single, 200-line file [gemm.cuh](gemm.cuh) which implements half-precision tensor core matrix multiplication, optimised for Turing (SM75) architecture. 

The implementation builds on top of CuTe from [CUTLASS](https://github.com/NVIDIA/cutlass), a low-level interface for tensor manipulation in CUDA. The code is well-commented and is meant to be easily readable (minimal CUDA/C++ background knowledge required) and hackable.

Benchmark against standard implementations (see [main.cu](main.cu) and [reference.cu](test/reference.cu)):
```
$ ./main
Usage: ./main M N K iters

$ ./main 4096 4096 4096 1000
Time elapse: 6043.59ms
TFLOPS: 22.7413

$ ./main 8192 8192 8192 100
Time elapse: 4819.51ms
TFLOPS: 22.8138

$ ./reference 4096 4096 4096 1000
Time elapse: 6040.42ms
TFLOPS: 22.7532

$ ./reference 8192 8192 8192 100
Time elapse: 4657.08ms
TFLOPS: 23.6095
```
> The theoretical maximum for the hardware I used (RTX 2060) is 26 TFLOPS.

## Quick start

> Requires CUDA installed. Checkout https://docs.nvidia.com/cuda/cuda-installation-guide-linux/ for instructions.
> If you don't have a compatible GPU, you can run this in Colab:
> <a target="_blank" href="https://colab.research.google.com/github/andylolu2/simpleGEMM/blob/master/colab/simpleGEMM.ipynb">
>  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
> </a>

Compile the [`main.cu`](main.cu) file:
```bash
nvcc \
    --include-path ./ \
    --include-path cutlass/include \
    --generate-code=arch=compute_75,code=[compute_75,sm_75] \
    --expt-relaxed-constexpr \
    -forward-unknown-to-host-compiler \
    -std=c++17 \
    -O3 \
    -o build/main \
    main.cu
```

And run!
```
$ ./build/main
Usage: ./main M N K iters

$ ./build/main 4096 4096 4096 1000
Time elapse: 6043.59ms
TFLOPS: 22.7413
```

You can also build with `CMake` (better option for development):
```bash
$ mkdir build
$ cd build/
$ cmake ..
-- Configuring done
-- Generating done
-- Build files have been written to: /workspaces/simpleGEMM/build
$ make main 
Consolidate compiler generated dependencies of target main
[ 50%] Building CUDA object CMakeFiles/main.dir/main.cu.o
[100%] Linking CUDA executable main
[100%] Built target main
$ ./main
Usage: ./main M N K iters
```

## What's missing

The code trades off generality for simplicity:
- Only supports half-precision matmul.
- Assumes (asserts) the inputs are divisible by the block size.
- Assumes the inputs are in row-major layout. (Though you probably only want to row-major layout anyway as other combinations are 10-30% slower.)
- Doesn't do software pipelining. (interleaving global memory load for the next tile with computation.)
- Is only optimal for "normal" problem sizes. For more exotic problem sizes like small-M/N with large-K, specialised implementations like split-K kernel is likely to perform better.
