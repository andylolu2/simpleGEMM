{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN/spycrdtzxaAzX0e2pBum",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andylolu2/simpleGEMM/blob/master/colab/simpleGEMM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwlrNKDABN0q",
        "outputId": "c04f6368-db2a-4e3a-819a-d352351f0598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'simpleGEMM'...\n",
            "remote: Enumerating objects: 41, done.\u001b[K\n",
            "remote: Counting objects: 100% (41/41), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 41 (delta 21), reused 18 (delta 8), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (41/41), 17.79 KiB | 2.96 MiB/s, done.\n",
            "Resolving deltas: 100% (21/21), done.\n",
            "Submodule 'src/cutlass' (https://github.com/NVIDIA/cutlass.git) registered for path 'cutlass'\n",
            "Cloning into '/content/simpleGEMM/cutlass'...\n",
            "remote: Enumerating objects: 26417, done.        \n",
            "remote: Counting objects: 100% (7329/7329), done.        \n",
            "remote: Compressing objects: 100% (984/984), done.        \n",
            "remote: Total 26417 (delta 6693), reused 6358 (delta 6345), pack-reused 19088        \n",
            "Receiving objects: 100% (26417/26417), 41.27 MiB | 21.09 MiB/s, done.\n",
            "Resolving deltas: 100% (20146/20146), done.\n",
            "Submodule path 'cutlass': checked out '19f3cc33f1642b490ed7126ea0141f79c0045527'\n",
            "/content/simpleGEMM\n"
          ]
        }
      ],
      "source": [
        "!git clone --recurse-submodules https://github.com/andylolu2/simpleGEMM.git\n",
        "%cd simpleGEMM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir build\n",
        "%cd build\n",
        "!cmake ..\n",
        "!make"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eMYclEXGXYf",
        "outputId": "660a9bba-8116-4f47-f954-e58538582ad9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/simpleGEMM/build\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- The CUDA compiler identification is NVIDIA 12.2.140\n",
            "-- Detecting CUDA compiler ABI info\n",
            "-- Detecting CUDA compiler ABI info - done\n",
            "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "-- Detecting CUDA compile features\n",
            "-- Detecting CUDA compile features - done\n",
            "-- Configuring done (4.8s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /content/simpleGEMM/build\n",
            "[ 16%] \u001b[32mBuilding CUDA object CMakeFiles/main.dir/main.cu.o\u001b[0m\n",
            "[ 33%] \u001b[32m\u001b[1mLinking CUDA executable main\u001b[0m\n",
            "[ 33%] Built target main\n",
            "[ 50%] \u001b[32mBuilding CUDA object CMakeFiles/reference.dir/test/reference.cu.o\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CUDA executable reference\u001b[0m\n",
            "[ 66%] Built target reference\n",
            "[ 83%] \u001b[32mBuilding CUDA object CMakeFiles/correctness.dir/test/correctness.cu.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CUDA executable correctness\u001b[0m\n",
            "[100%] Built target correctness\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"Ours:\"\n",
        "!./main 4096 4096 4096 1000\n",
        "!echo \"Reference:\"\n",
        "!./reference 4096 4096 4096 1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8sW6zRVGgUI",
        "outputId": "dc70e760-6d7d-426b-8c05-b71cded5f353"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ours:\n",
            "Time elapse: 3984.36ms\n",
            "TFLOPS: 34.4946\n",
            "Reference:\n",
            "Time elapse: 3942.95ms\n",
            "TFLOPS: 34.8568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare against PyTorch (CuBLAS)\n",
        "import torch\n",
        "\n",
        "M, N, K = 4096, 4096, 4096\n",
        "iters = 1000\n",
        "\n",
        "A = torch.randn(M, K, dtype=torch.float16, device=\"cuda\")\n",
        "B = torch.randn(N, K, dtype=torch.float16, device=\"cuda\").T\n",
        "\n",
        "start = torch.cuda.Event(enable_timing=True)\n",
        "end = torch.cuda.Event(enable_timing=True)\n",
        "start.record()\n",
        "for _ in range(iters):\n",
        "    C = A @ B\n",
        "end.record()\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "total_duration = start.elapsed_time(end)  # in ms\n",
        "tflops = 2 * M * N * K * iters / (total_duration / 1000) / 1e12\n",
        "print(f\"Time elapse: {total_duration:.2f}ms\")\n",
        "print(f\"TFLOPS: {tflops:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znxkM_A7Cwqy",
        "outputId": "a200a7be-3144-4454-b808-ed7a6bb69e12"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time elapse: 6606.95ms\n",
            "TFLOPS: 20.8022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> TODO: Why is PyTorch so slow?"
      ],
      "metadata": {
        "id": "OXw0Wv8rEqxG"
      }
    }
  ]
}